{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "lstm seq2seq를 이용한 오타교정기",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "S03PUDI3e7mf"
      },
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, LSTM, Dense\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWoQJoYAe7mk"
      },
      "source": [
        "batch_size = 64  # Batch size for training.\n",
        "epochs = 100  # Number of epochs to train for.\n",
        "latent_dim = 256  # Latent dimensionality of the encoding space.\n",
        "num_samples = 10000  # Number of samples to train on.\n",
        "# Path to the data txt file on disk."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuMuLEiee7mk"
      },
      "source": [
        "# data_path = 'fra.txt'\n",
        "# input_texts = []\n",
        "# target_texts = []\n",
        "# input_characters = set()\n",
        "# target_characters = set()\n",
        "# with open(data_path, 'r', encoding='utf-8') as f:\n",
        "#     lines = f.read().split('\\n')\n",
        "# for line in lines[: min(num_samples, len(lines) - 1)]:\n",
        "#     input_text, target_text, _ = line.split('\\t')\n",
        "#     target_text = '\\t' + target_text + '\\n'\n",
        "#     input_texts.append(input_text)\n",
        "#     target_texts.append(target_text)\n",
        "#     for char in input_text:\n",
        "#         if char not in input_characters:\n",
        "#             input_characters.add(char)\n",
        "#     for char in target_text:\n",
        "#         if char not in target_characters:\n",
        "#             target_characters.add(char)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2cXEMUIf5zN"
      },
      "source": [
        "data_path = '/content/오탈자.csv'\r\n",
        "data = pd.read_csv(data_path, encoding='utf-8')\r\n",
        "input_characters = set()\r\n",
        "target_characters = set()\r\n",
        "input_characters.add(' ')\r\n",
        "target_characters.add(' ')\r\n",
        "target_characters.add('<START>')\r\n",
        "target_characters.add('<END>')\r\n",
        "input_texts = data[\"Wrong\"]\r\n",
        "target_texts = '<START> ' + data[\"Right\"] + ' <END>'\r\n",
        "\r\n",
        "### Get English and Hindi Vocabulary\r\n",
        "all_eng_words=set()\r\n",
        "for eng in data['Wrong']:\r\n",
        "    for word in eng.split():\r\n",
        "        if word not in input_characters:\r\n",
        "            input_characters.add(word)\r\n",
        "\r\n",
        "all_hindi_words=set()\r\n",
        "for hin in data['Right']:\r\n",
        "    for word in hin.split():\r\n",
        "        if word not in target_characters:\r\n",
        "            target_characters.add(word)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "FR2U4-E46aY3",
        "outputId": "58db7742-66d0-42f8-aea8-fadca72fa7a2"
      },
      "source": [
        "data.tail()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Wrong</th>\n",
              "      <th>Right</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>방방곳곳</td>\n",
              "      <td>방방곡곡</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>성대묘사</td>\n",
              "      <td>성대모사</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>소매깃</td>\n",
              "      <td>소맷귀</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>세 살박이</td>\n",
              "      <td>세 살배기</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>쉽상</td>\n",
              "      <td>십상</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Wrong  Right\n",
              "36   방방곳곳   방방곡곡\n",
              "37   성대묘사   성대모사\n",
              "38    소매깃    소맷귀\n",
              "39  세 살박이  세 살배기\n",
              "40     쉽상     십상"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7CuFawpiG2Z",
        "outputId": "d32e2e7a-1b20-42c1-b6ee-88b61f17ae2b"
      },
      "source": [
        "input_characters"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ',\n",
              " '?',\n",
              " '가는',\n",
              " '감자',\n",
              " '개거품',\n",
              " '결제했다.',\n",
              " '곳',\n",
              " '관련',\n",
              " '구렛나루',\n",
              " '궁시렁거리지',\n",
              " '그',\n",
              " '그만',\n",
              " '금새',\n",
              " '기다리면',\n",
              " '깨방정',\n",
              " '꺼야!',\n",
              " '끼여들기',\n",
              " '나리',\n",
              " '나중에',\n",
              " '날',\n",
              " '났다.',\n",
              " '낳다.',\n",
              " '내',\n",
              " '내가',\n",
              " '내노라하는',\n",
              " '내놓라하는',\n",
              " '너',\n",
              " '넌',\n",
              " '널판지',\n",
              " '담배를',\n",
              " '대고',\n",
              " '더종비즈온',\n",
              " '동맹이',\n",
              " '된대',\n",
              " '들었니?',\n",
              " '떨어라',\n",
              " '만나',\n",
              " '만나는구나~',\n",
              " '말대꾸야!',\n",
              " '몇일만',\n",
              " '모임이',\n",
              " '믾이',\n",
              " '바람에',\n",
              " '반ㄴ갑습니다.',\n",
              " '방방곳곳',\n",
              " '밪으세요.',\n",
              " '병이',\n",
              " '복',\n",
              " '뵈요.',\n",
              " '불리운다.',\n",
              " '비가',\n",
              " '사단이',\n",
              " '사장이',\n",
              " '살박이',\n",
              " '새해',\n",
              " '서류를',\n",
              " '설겆이',\n",
              " '성대묘사',\n",
              " '세',\n",
              " '소매깃',\n",
              " '쉽상',\n",
              " '안ㄴ녕하세요.',\n",
              " '알았다.',\n",
              " '양년',\n",
              " '양년치킨',\n",
              " '어따',\n",
              " '어떻해',\n",
              " '어음결재',\n",
              " '어의가',\n",
              " '언제니',\n",
              " '업습니다.',\n",
              " '없네',\n",
              " '예기',\n",
              " '오는',\n",
              " '오랫만에',\n",
              " '왠만하면',\n",
              " '이',\n",
              " '좀',\n",
              " '좋은',\n",
              " '줄',\n",
              " '창워느로',\n",
              " '천재라고',\n",
              " '출발하겠습니다.',\n",
              " '출짱',\n",
              " '파토가',\n",
              " '피다.',\n",
              " '회사의'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2ZvgSCfe7ml"
      },
      "source": [
        "input_characters = sorted(list(input_characters))\n",
        "target_characters = sorted(list(target_characters))\n",
        "num_encoder_tokens = len(input_characters)\n",
        "num_decoder_tokens = len(target_characters)\n",
        "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
        "max_decoder_seq_length = max([len(txt) for txt in target_texts])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjxqyMv9e7ml",
        "outputId": "791644e2-d375-492d-c527-8ded034b9eac"
      },
      "source": [
        "print('Number of samples:', len(input_texts))\n",
        "print('Number of unique input tokens:', num_encoder_tokens)\n",
        "print('Number of unique output tokens:', num_decoder_tokens)\n",
        "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
        "print('Max sequence length for outputs:', max_decoder_seq_length)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of samples: 41\n",
            "Number of unique input tokens: 87\n",
            "Number of unique output tokens: 88\n",
            "Max sequence length for inputs: 25\n",
            "Max sequence length for outputs: 39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPgrweFEe7ml"
      },
      "source": [
        "input_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(input_characters)])\n",
        "target_token_index = dict(\n",
        "    [(char, i) for i, char in enumerate(target_characters)])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTcD0q7ve7mm"
      },
      "source": [
        "encoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_input_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')\n",
        "decoder_target_data = np.zeros(\n",
        "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
        "    dtype='float32')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTfsAQxowgFG",
        "outputId": "30b0fa6c-a0e8-41e1-f65a-5a2d953fe706"
      },
      "source": [
        "input_token_index"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{' ': 0,\n",
              " '?': 1,\n",
              " '가는': 2,\n",
              " '감자': 3,\n",
              " '개거품': 4,\n",
              " '결제했다.': 5,\n",
              " '곳': 6,\n",
              " '관련': 7,\n",
              " '구렛나루': 8,\n",
              " '궁시렁거리지': 9,\n",
              " '그': 10,\n",
              " '그만': 11,\n",
              " '금새': 12,\n",
              " '기다리면': 13,\n",
              " '깨방정': 14,\n",
              " '꺼야!': 15,\n",
              " '끼여들기': 16,\n",
              " '나리': 17,\n",
              " '나중에': 18,\n",
              " '날': 19,\n",
              " '났다.': 20,\n",
              " '낳다.': 21,\n",
              " '내': 22,\n",
              " '내가': 23,\n",
              " '내노라하는': 24,\n",
              " '내놓라하는': 25,\n",
              " '너': 26,\n",
              " '넌': 27,\n",
              " '널판지': 28,\n",
              " '담배를': 29,\n",
              " '대고': 30,\n",
              " '더종비즈온': 31,\n",
              " '동맹이': 32,\n",
              " '된대': 33,\n",
              " '들었니?': 34,\n",
              " '떨어라': 35,\n",
              " '만나': 36,\n",
              " '만나는구나~': 37,\n",
              " '말대꾸야!': 38,\n",
              " '몇일만': 39,\n",
              " '모임이': 40,\n",
              " '믾이': 41,\n",
              " '바람에': 42,\n",
              " '반ㄴ갑습니다.': 43,\n",
              " '방방곳곳': 44,\n",
              " '밪으세요.': 45,\n",
              " '병이': 46,\n",
              " '복': 47,\n",
              " '뵈요.': 48,\n",
              " '불리운다.': 49,\n",
              " '비가': 50,\n",
              " '사단이': 51,\n",
              " '사장이': 52,\n",
              " '살박이': 53,\n",
              " '새해': 54,\n",
              " '서류를': 55,\n",
              " '설겆이': 56,\n",
              " '성대묘사': 57,\n",
              " '세': 58,\n",
              " '소매깃': 59,\n",
              " '쉽상': 60,\n",
              " '안ㄴ녕하세요.': 61,\n",
              " '알았다.': 62,\n",
              " '양년': 63,\n",
              " '양년치킨': 64,\n",
              " '어따': 65,\n",
              " '어떻해': 66,\n",
              " '어음결재': 67,\n",
              " '어의가': 68,\n",
              " '언제니': 69,\n",
              " '업습니다.': 70,\n",
              " '없네': 71,\n",
              " '예기': 72,\n",
              " '오는': 73,\n",
              " '오랫만에': 74,\n",
              " '왠만하면': 75,\n",
              " '이': 76,\n",
              " '좀': 77,\n",
              " '좋은': 78,\n",
              " '줄': 79,\n",
              " '창워느로': 80,\n",
              " '천재라고': 81,\n",
              " '출발하겠습니다.': 82,\n",
              " '출짱': 83,\n",
              " '파토가': 84,\n",
              " '피다.': 85,\n",
              " '회사의': 86}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGAmRh2we7mm"
      },
      "source": [
        "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
        "    for t, char in enumerate(input_text.split()):\n",
        "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
        "    encoder_input_data[i, t + 1:, input_token_index[\" \"]] = 1.\n",
        "    for t, char in enumerate(target_text.split()):\n",
        "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
        "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
        "        if t > 0:\n",
        "            # decoder_target_data will be ahead by one timestep\n",
        "            # and will not include the start character.\n",
        "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
        "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
        "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ow7Xdire7mm"
      },
      "source": [
        "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
        "encoder = LSTM(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
        "encoder_states = [state_h, state_c]\n",
        "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
        "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,initial_state=encoder_states)\n",
        "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ulW_9VgQe7mm",
        "outputId": "8dc6b241-6e80-4357-d289-9bc83dece920"
      },
      "source": [
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=batch_size, epochs=300, validation_split=0.2)\n",
        "# Save model\n",
        "model.save('s2s.h5')"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n",
            "1/1 [==============================] - 5s 5s/step - loss: 4.4549 - accuracy: 8.0128e-04 - val_loss: 3.7601 - val_accuracy: 0.9459\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 3.7934 - accuracy: 0.9127 - val_loss: 0.8029 - val_accuracy: 0.9459\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.9196 - accuracy: 0.9127 - val_loss: 0.2714 - val_accuracy: 0.9459\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.5244 - accuracy: 0.9127 - val_loss: 0.3280 - val_accuracy: 0.9459\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.4498 - accuracy: 0.9127 - val_loss: 0.2296 - val_accuracy: 0.9459\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.4292 - accuracy: 0.9127 - val_loss: 0.2679 - val_accuracy: 0.9687\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.4066 - accuracy: 0.9239 - val_loss: 0.2360 - val_accuracy: 0.9687\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.3997 - accuracy: 0.9239 - val_loss: 0.2586 - val_accuracy: 0.9715\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.3939 - accuracy: 0.9271 - val_loss: 0.2393 - val_accuracy: 0.9687\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.3898 - accuracy: 0.9239 - val_loss: 0.2609 - val_accuracy: 0.9487\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.3860 - accuracy: 0.9175 - val_loss: 0.2396 - val_accuracy: 0.9687\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.3833 - accuracy: 0.9239 - val_loss: 0.2696 - val_accuracy: 0.9487\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.3808 - accuracy: 0.9175 - val_loss: 0.2362 - val_accuracy: 0.9687\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.3808 - accuracy: 0.9239 - val_loss: 0.2870 - val_accuracy: 0.9487\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.3797 - accuracy: 0.9175 - val_loss: 0.2325 - val_accuracy: 0.9459\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.3838 - accuracy: 0.9127 - val_loss: 0.2972 - val_accuracy: 0.9487\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.3774 - accuracy: 0.9175 - val_loss: 0.2338 - val_accuracy: 0.9687\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.3781 - accuracy: 0.9239 - val_loss: 0.2911 - val_accuracy: 0.9487\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.3692 - accuracy: 0.9175 - val_loss: 0.2398 - val_accuracy: 0.9687\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 0.3673 - accuracy: 0.9239 - val_loss: 0.2878 - val_accuracy: 0.9487\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.3620 - accuracy: 0.9175 - val_loss: 0.2444 - val_accuracy: 0.9687\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.3605 - accuracy: 0.9239 - val_loss: 0.2916 - val_accuracy: 0.9487\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.3570 - accuracy: 0.9175 - val_loss: 0.2458 - val_accuracy: 0.9687\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.3567 - accuracy: 0.9239 - val_loss: 0.3011 - val_accuracy: 0.9487\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.3540 - accuracy: 0.9175 - val_loss: 0.2453 - val_accuracy: 0.9687\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.3554 - accuracy: 0.9239 - val_loss: 0.3105 - val_accuracy: 0.9487\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.3515 - accuracy: 0.9175 - val_loss: 0.2457 - val_accuracy: 0.9687\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.3529 - accuracy: 0.9239 - val_loss: 0.3121 - val_accuracy: 0.9487\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.3463 - accuracy: 0.9191 - val_loss: 0.2497 - val_accuracy: 0.9687\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.3459 - accuracy: 0.9239 - val_loss: 0.3099 - val_accuracy: 0.9487\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.3399 - accuracy: 0.9191 - val_loss: 0.2551 - val_accuracy: 0.9687\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.3391 - accuracy: 0.9239 - val_loss: 0.3114 - val_accuracy: 0.9487\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.3347 - accuracy: 0.9191 - val_loss: 0.2580 - val_accuracy: 0.9687\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.3346 - accuracy: 0.9255 - val_loss: 0.3182 - val_accuracy: 0.9487\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.3311 - accuracy: 0.9191 - val_loss: 0.2574 - val_accuracy: 0.9687\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.3322 - accuracy: 0.9255 - val_loss: 0.3268 - val_accuracy: 0.9487\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.3284 - accuracy: 0.9191 - val_loss: 0.2581 - val_accuracy: 0.9687\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.3301 - accuracy: 0.9255 - val_loss: 0.3283 - val_accuracy: 0.9487\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.3241 - accuracy: 0.9191 - val_loss: 0.2644 - val_accuracy: 0.9687\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.3244 - accuracy: 0.9255 - val_loss: 0.3275 - val_accuracy: 0.9487\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.3191 - accuracy: 0.9191 - val_loss: 0.2699 - val_accuracy: 0.9687\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.3191 - accuracy: 0.9255 - val_loss: 0.3292 - val_accuracy: 0.9487\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.3150 - accuracy: 0.9191 - val_loss: 0.2744 - val_accuracy: 0.9687\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.3158 - accuracy: 0.9255 - val_loss: 0.3331 - val_accuracy: 0.9487\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.3120 - accuracy: 0.9191 - val_loss: 0.2780 - val_accuracy: 0.9687\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.3134 - accuracy: 0.9255 - val_loss: 0.3381 - val_accuracy: 0.9487\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.3091 - accuracy: 0.9191 - val_loss: 0.2763 - val_accuracy: 0.9687\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.3109 - accuracy: 0.9255 - val_loss: 0.3473 - val_accuracy: 0.9487\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.3070 - accuracy: 0.9303 - val_loss: 0.2754 - val_accuracy: 0.9687\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 0s 489ms/step - loss: 0.3091 - accuracy: 0.9255 - val_loss: 0.3447 - val_accuracy: 0.9715\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.3020 - accuracy: 0.9303 - val_loss: 0.2843 - val_accuracy: 0.9687\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.3015 - accuracy: 0.9255 - val_loss: 0.3437 - val_accuracy: 0.9715\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2978 - accuracy: 0.9303 - val_loss: 0.2913 - val_accuracy: 0.9687\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2999 - accuracy: 0.9255 - val_loss: 0.3496 - val_accuracy: 0.9487\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2970 - accuracy: 0.9191 - val_loss: 0.2985 - val_accuracy: 0.9687\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.3001 - accuracy: 0.9255 - val_loss: 0.3471 - val_accuracy: 0.9487\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2894 - accuracy: 0.9303 - val_loss: 0.2959 - val_accuracy: 0.9687\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2945 - accuracy: 0.9223 - val_loss: 0.4009 - val_accuracy: 0.9487\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.3073 - accuracy: 0.9191 - val_loss: 0.2873 - val_accuracy: 0.9687\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.3045 - accuracy: 0.9255 - val_loss: 0.3534 - val_accuracy: 0.9687\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2865 - accuracy: 0.9303 - val_loss: 0.3090 - val_accuracy: 0.9687\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2836 - accuracy: 0.9255 - val_loss: 0.3453 - val_accuracy: 0.9687\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2797 - accuracy: 0.9303 - val_loss: 0.3152 - val_accuracy: 0.9687\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2786 - accuracy: 0.9255 - val_loss: 0.3502 - val_accuracy: 0.9715\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2774 - accuracy: 0.9303 - val_loss: 0.3148 - val_accuracy: 0.9687\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2801 - accuracy: 0.9255 - val_loss: 0.3653 - val_accuracy: 0.9715\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2801 - accuracy: 0.9303 - val_loss: 0.3136 - val_accuracy: 0.9687\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 0.2857 - accuracy: 0.9255 - val_loss: 0.3676 - val_accuracy: 0.9715\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2750 - accuracy: 0.9303 - val_loss: 0.3059 - val_accuracy: 0.9687\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2792 - accuracy: 0.9255 - val_loss: 0.3926 - val_accuracy: 0.9687\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2807 - accuracy: 0.9303 - val_loss: 0.3028 - val_accuracy: 0.9687\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2833 - accuracy: 0.9255 - val_loss: 0.3719 - val_accuracy: 0.9687\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 0s 289ms/step - loss: 0.2691 - accuracy: 0.9303 - val_loss: 0.3164 - val_accuracy: 0.9687\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2682 - accuracy: 0.9255 - val_loss: 0.3741 - val_accuracy: 0.9715\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 0.2648 - accuracy: 0.9303 - val_loss: 0.3176 - val_accuracy: 0.9687\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.2680 - accuracy: 0.9255 - val_loss: 0.3845 - val_accuracy: 0.9487\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2582 - accuracy: 0.9311 - val_loss: 0.2989 - val_accuracy: 0.9687\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.3603 - accuracy: 0.9135 - val_loss: 0.4161 - val_accuracy: 0.9715\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2763 - accuracy: 0.9295 - val_loss: 0.3205 - val_accuracy: 0.9687\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.2676 - accuracy: 0.9255 - val_loss: 0.3754 - val_accuracy: 0.9687\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.2562 - accuracy: 0.9303 - val_loss: 0.3340 - val_accuracy: 0.9687\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2528 - accuracy: 0.9255 - val_loss: 0.3684 - val_accuracy: 0.9687\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.2501 - accuracy: 0.9303 - val_loss: 0.3352 - val_accuracy: 0.9687\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.2505 - accuracy: 0.9303 - val_loss: 0.3822 - val_accuracy: 0.9687\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2537 - accuracy: 0.9359 - val_loss: 0.3338 - val_accuracy: 0.9687\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2647 - accuracy: 0.9303 - val_loss: 0.3812 - val_accuracy: 0.9687\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 0.2494 - accuracy: 0.9335 - val_loss: 0.3334 - val_accuracy: 0.9687\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.2496 - accuracy: 0.9255 - val_loss: 0.3894 - val_accuracy: 0.9715\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2462 - accuracy: 0.9351 - val_loss: 0.3232 - val_accuracy: 0.9687\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.2522 - accuracy: 0.9255 - val_loss: 0.4117 - val_accuracy: 0.9687\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 0s 294ms/step - loss: 0.2538 - accuracy: 0.9311 - val_loss: 0.3225 - val_accuracy: 0.9687\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.2544 - accuracy: 0.9287 - val_loss: 0.3861 - val_accuracy: 0.9687\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 0s 291ms/step - loss: 0.2410 - accuracy: 0.9319 - val_loss: 0.3346 - val_accuracy: 0.9687\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.2396 - accuracy: 0.9303 - val_loss: 0.3887 - val_accuracy: 0.9687\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2404 - accuracy: 0.9399 - val_loss: 0.3394 - val_accuracy: 0.9687\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2515 - accuracy: 0.9255 - val_loss: 0.3874 - val_accuracy: 0.9715\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.2338 - accuracy: 0.9399 - val_loss: 0.3371 - val_accuracy: 0.9687\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2337 - accuracy: 0.9255 - val_loss: 0.3906 - val_accuracy: 0.9715\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 0s 481ms/step - loss: 0.2275 - accuracy: 0.9399 - val_loss: 0.3264 - val_accuracy: 0.9687\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.2311 - accuracy: 0.9255 - val_loss: 0.4236 - val_accuracy: 0.9687\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2394 - accuracy: 0.9511 - val_loss: 0.3194 - val_accuracy: 0.9687\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.2465 - accuracy: 0.9295 - val_loss: 0.4063 - val_accuracy: 0.9687\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.2303 - accuracy: 0.9487 - val_loss: 0.3342 - val_accuracy: 0.9687\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2237 - accuracy: 0.9303 - val_loss: 0.3887 - val_accuracy: 0.9687\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.2167 - accuracy: 0.9399 - val_loss: 0.3358 - val_accuracy: 0.9687\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.2156 - accuracy: 0.9255 - val_loss: 0.3975 - val_accuracy: 0.9715\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.2133 - accuracy: 0.9399 - val_loss: 0.3219 - val_accuracy: 0.9687\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2167 - accuracy: 0.9255 - val_loss: 0.4376 - val_accuracy: 0.9715\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.2258 - accuracy: 0.9511 - val_loss: 0.3223 - val_accuracy: 0.9687\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.2248 - accuracy: 0.9255 - val_loss: 0.4236 - val_accuracy: 0.9687\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.2168 - accuracy: 0.9615 - val_loss: 0.3378 - val_accuracy: 0.9687\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.2100 - accuracy: 0.9351 - val_loss: 0.3917 - val_accuracy: 0.9687\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.1994 - accuracy: 0.9599 - val_loss: 0.3347 - val_accuracy: 0.9687\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.1948 - accuracy: 0.9367 - val_loss: 0.4291 - val_accuracy: 0.9715\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.2036 - accuracy: 0.9591 - val_loss: 0.3421 - val_accuracy: 0.9687\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.1957 - accuracy: 0.9287 - val_loss: 0.4230 - val_accuracy: 0.9715\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.1945 - accuracy: 0.9583 - val_loss: 0.3374 - val_accuracy: 0.9687\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.1985 - accuracy: 0.9263 - val_loss: 0.4737 - val_accuracy: 0.9487\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.2202 - accuracy: 0.9575 - val_loss: 0.3634 - val_accuracy: 0.9687\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 0s 329ms/step - loss: 0.2051 - accuracy: 0.9255 - val_loss: 0.4183 - val_accuracy: 0.9715\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.1930 - accuracy: 0.9551 - val_loss: 0.3681 - val_accuracy: 0.9687\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.1945 - accuracy: 0.9327 - val_loss: 0.4040 - val_accuracy: 0.9687\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.1862 - accuracy: 0.9623 - val_loss: 0.3535 - val_accuracy: 0.9687\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.1826 - accuracy: 0.9399 - val_loss: 0.4551 - val_accuracy: 0.9715\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.1947 - accuracy: 0.9728 - val_loss: 0.3577 - val_accuracy: 0.9715\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.1893 - accuracy: 0.9399 - val_loss: 0.4357 - val_accuracy: 0.9687\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 0s 292ms/step - loss: 0.1813 - accuracy: 0.9752 - val_loss: 0.3578 - val_accuracy: 0.9687\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.1908 - accuracy: 0.9423 - val_loss: 0.4766 - val_accuracy: 0.9715\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.2016 - accuracy: 0.9720 - val_loss: 0.3796 - val_accuracy: 0.9687\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 0s 299ms/step - loss: 0.1898 - accuracy: 0.9351 - val_loss: 0.4322 - val_accuracy: 0.9715\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.1819 - accuracy: 0.9655 - val_loss: 0.3825 - val_accuracy: 0.9687\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.1797 - accuracy: 0.9359 - val_loss: 0.4245 - val_accuracy: 0.9687\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.1810 - accuracy: 0.9655 - val_loss: 0.3783 - val_accuracy: 0.9715\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.1843 - accuracy: 0.9375 - val_loss: 0.4281 - val_accuracy: 0.9687\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 0.1784 - accuracy: 0.9688 - val_loss: 0.3638 - val_accuracy: 0.9687\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.1727 - accuracy: 0.9439 - val_loss: 0.4410 - val_accuracy: 0.9715\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.1718 - accuracy: 0.9720 - val_loss: 0.3582 - val_accuracy: 0.9687\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.1703 - accuracy: 0.9503 - val_loss: 0.4521 - val_accuracy: 0.9715\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.1696 - accuracy: 0.9752 - val_loss: 0.3609 - val_accuracy: 0.9687\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.1635 - accuracy: 0.9583 - val_loss: 0.4395 - val_accuracy: 0.9715\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.1589 - accuracy: 0.9728 - val_loss: 0.3679 - val_accuracy: 0.9687\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.1526 - accuracy: 0.9631 - val_loss: 0.4359 - val_accuracy: 0.9715\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.1522 - accuracy: 0.9728 - val_loss: 0.3723 - val_accuracy: 0.9687\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.1491 - accuracy: 0.9639 - val_loss: 0.4545 - val_accuracy: 0.9715\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.1553 - accuracy: 0.9768 - val_loss: 0.3792 - val_accuracy: 0.9687\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.1496 - accuracy: 0.9591 - val_loss: 0.4510 - val_accuracy: 0.9687\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.1558 - accuracy: 0.9768 - val_loss: 0.3847 - val_accuracy: 0.9487\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.1725 - accuracy: 0.9447 - val_loss: 0.5050 - val_accuracy: 0.9459\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.1886 - accuracy: 0.9736 - val_loss: 0.3890 - val_accuracy: 0.9687\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.1640 - accuracy: 0.9399 - val_loss: 0.4553 - val_accuracy: 0.9715\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.1537 - accuracy: 0.9744 - val_loss: 0.3876 - val_accuracy: 0.9687\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.1424 - accuracy: 0.9655 - val_loss: 0.4318 - val_accuracy: 0.9715\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.1353 - accuracy: 0.9744 - val_loss: 0.3915 - val_accuracy: 0.9687\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.1323 - accuracy: 0.9712 - val_loss: 0.4545 - val_accuracy: 0.9715\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.1366 - accuracy: 0.9768 - val_loss: 0.3847 - val_accuracy: 0.9687\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 0.1343 - accuracy: 0.9696 - val_loss: 0.4794 - val_accuracy: 0.9715\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.1397 - accuracy: 0.9768 - val_loss: 0.3878 - val_accuracy: 0.9687\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.1368 - accuracy: 0.9647 - val_loss: 0.4922 - val_accuracy: 0.9715\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 0s 482ms/step - loss: 0.1453 - accuracy: 0.9752 - val_loss: 0.3960 - val_accuracy: 0.9687\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.1369 - accuracy: 0.9639 - val_loss: 0.4646 - val_accuracy: 0.9715\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.1290 - accuracy: 0.9768 - val_loss: 0.4005 - val_accuracy: 0.9715\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.1287 - accuracy: 0.9720 - val_loss: 0.4738 - val_accuracy: 0.9715\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.1378 - accuracy: 0.9768 - val_loss: 0.3989 - val_accuracy: 0.9715\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.1362 - accuracy: 0.9615 - val_loss: 0.4792 - val_accuracy: 0.9687\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.1422 - accuracy: 0.9768 - val_loss: 0.3976 - val_accuracy: 0.9715\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.1407 - accuracy: 0.9551 - val_loss: 0.4905 - val_accuracy: 0.9687\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.1455 - accuracy: 0.9768 - val_loss: 0.4006 - val_accuracy: 0.9687\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.1400 - accuracy: 0.9615 - val_loss: 0.5079 - val_accuracy: 0.9715\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.1452 - accuracy: 0.9728 - val_loss: 0.4055 - val_accuracy: 0.9687\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.1284 - accuracy: 0.9639 - val_loss: 0.4681 - val_accuracy: 0.9715\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.1199 - accuracy: 0.9768 - val_loss: 0.4098 - val_accuracy: 0.9687\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.1128 - accuracy: 0.9728 - val_loss: 0.4463 - val_accuracy: 0.9715\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.1098 - accuracy: 0.9768 - val_loss: 0.4199 - val_accuracy: 0.9715\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.1086 - accuracy: 0.9776 - val_loss: 0.4569 - val_accuracy: 0.9715\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.1099 - accuracy: 0.9768 - val_loss: 0.4104 - val_accuracy: 0.9715\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.1099 - accuracy: 0.9736 - val_loss: 0.4906 - val_accuracy: 0.9715\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.1168 - accuracy: 0.9768 - val_loss: 0.4092 - val_accuracy: 0.9715\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.1169 - accuracy: 0.9720 - val_loss: 0.5115 - val_accuracy: 0.9487\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 0s 344ms/step - loss: 0.1264 - accuracy: 0.9752 - val_loss: 0.4202 - val_accuracy: 0.9715\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.1168 - accuracy: 0.9647 - val_loss: 0.4898 - val_accuracy: 0.9658\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.1102 - accuracy: 0.9768 - val_loss: 0.4331 - val_accuracy: 0.9715\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.1132 - accuracy: 0.9768 - val_loss: 0.4789 - val_accuracy: 0.9715\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.1157 - accuracy: 0.9784 - val_loss: 0.4152 - val_accuracy: 0.9687\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.1199 - accuracy: 0.9639 - val_loss: 0.5471 - val_accuracy: 0.9487\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 0.1400 - accuracy: 0.9752 - val_loss: 0.4236 - val_accuracy: 0.9687\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 0.1242 - accuracy: 0.9639 - val_loss: 0.4791 - val_accuracy: 0.9687\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.1148 - accuracy: 0.9768 - val_loss: 0.4269 - val_accuracy: 0.9715\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.1061 - accuracy: 0.9760 - val_loss: 0.4431 - val_accuracy: 0.9687\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.1014 - accuracy: 0.9776 - val_loss: 0.4238 - val_accuracy: 0.9715\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.0976 - accuracy: 0.9776 - val_loss: 0.4510 - val_accuracy: 0.9715\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.0962 - accuracy: 0.9776 - val_loss: 0.4173 - val_accuracy: 0.9687\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 0s 324ms/step - loss: 0.0955 - accuracy: 0.9776 - val_loss: 0.4784 - val_accuracy: 0.9715\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.0988 - accuracy: 0.9768 - val_loss: 0.4151 - val_accuracy: 0.9687\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.0985 - accuracy: 0.9760 - val_loss: 0.5036 - val_accuracy: 0.9715\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.1054 - accuracy: 0.9752 - val_loss: 0.4376 - val_accuracy: 0.9715\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.0968 - accuracy: 0.9768 - val_loss: 0.4693 - val_accuracy: 0.9715\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.0939 - accuracy: 0.9768 - val_loss: 0.4510 - val_accuracy: 0.9715\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.0911 - accuracy: 0.9784 - val_loss: 0.4671 - val_accuracy: 0.9715\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.0901 - accuracy: 0.9776 - val_loss: 0.4501 - val_accuracy: 0.9715\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.0892 - accuracy: 0.9768 - val_loss: 0.4757 - val_accuracy: 0.9715\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.0907 - accuracy: 0.9776 - val_loss: 0.4451 - val_accuracy: 0.9715\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 0s 302ms/step - loss: 0.0906 - accuracy: 0.9768 - val_loss: 0.5051 - val_accuracy: 0.9715\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.0992 - accuracy: 0.9752 - val_loss: 0.4374 - val_accuracy: 0.9715\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 0s 323ms/step - loss: 0.1045 - accuracy: 0.9712 - val_loss: 0.5551 - val_accuracy: 0.9459\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 0s 305ms/step - loss: 0.1091 - accuracy: 0.9768 - val_loss: 0.4635 - val_accuracy: 0.9715\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 2.8023 - accuracy: 0.5353 - val_loss: 0.4543 - val_accuracy: 0.9715\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 0s 471ms/step - loss: 0.0955 - accuracy: 0.9768 - val_loss: 0.4562 - val_accuracy: 0.9715\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.0865 - accuracy: 0.9784 - val_loss: 0.4397 - val_accuracy: 0.9687\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 0s 297ms/step - loss: 0.0845 - accuracy: 0.9784 - val_loss: 0.4546 - val_accuracy: 0.9715\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.0834 - accuracy: 0.9792 - val_loss: 0.4338 - val_accuracy: 0.9687\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.0828 - accuracy: 0.9784 - val_loss: 0.4632 - val_accuracy: 0.9715\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.0832 - accuracy: 0.9776 - val_loss: 0.4197 - val_accuracy: 0.9687\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.0854 - accuracy: 0.9776 - val_loss: 0.4934 - val_accuracy: 0.9715\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.0934 - accuracy: 0.9752 - val_loss: 0.4109 - val_accuracy: 0.9687\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.0917 - accuracy: 0.9728 - val_loss: 0.4996 - val_accuracy: 0.9715\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 0s 296ms/step - loss: 0.0919 - accuracy: 0.9768 - val_loss: 0.4302 - val_accuracy: 0.9687\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 0s 336ms/step - loss: 0.0827 - accuracy: 0.9768 - val_loss: 0.4661 - val_accuracy: 0.9715\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.0797 - accuracy: 0.9776 - val_loss: 0.4401 - val_accuracy: 0.9715\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.0780 - accuracy: 0.9800 - val_loss: 0.4523 - val_accuracy: 0.9715\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 0.0773 - accuracy: 0.9776 - val_loss: 0.4397 - val_accuracy: 0.9715\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.0765 - accuracy: 0.9800 - val_loss: 0.4527 - val_accuracy: 0.9715\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.0766 - accuracy: 0.9776 - val_loss: 0.4330 - val_accuracy: 0.9687\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.0762 - accuracy: 0.9784 - val_loss: 0.4636 - val_accuracy: 0.9715\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.0777 - accuracy: 0.9776 - val_loss: 0.4166 - val_accuracy: 0.9687\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.0783 - accuracy: 0.9784 - val_loss: 0.5024 - val_accuracy: 0.9715\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.0870 - accuracy: 0.9752 - val_loss: 0.3754 - val_accuracy: 0.9687\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.0938 - accuracy: 0.9728 - val_loss: 0.5662 - val_accuracy: 0.9487\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.1074 - accuracy: 0.9752 - val_loss: 0.4451 - val_accuracy: 0.9687\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.0868 - accuracy: 0.9744 - val_loss: 0.5016 - val_accuracy: 0.9487\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.0770 - accuracy: 0.9792 - val_loss: 0.4593 - val_accuracy: 0.9715\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.0728 - accuracy: 0.9800 - val_loss: 0.4647 - val_accuracy: 0.9715\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.0718 - accuracy: 0.9800 - val_loss: 0.4622 - val_accuracy: 0.9715\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.0712 - accuracy: 0.9800 - val_loss: 0.4645 - val_accuracy: 0.9715\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.0709 - accuracy: 0.9824 - val_loss: 0.4600 - val_accuracy: 0.9715\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 0s 332ms/step - loss: 0.0717 - accuracy: 0.9784 - val_loss: 0.4596 - val_accuracy: 0.9715\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.0722 - accuracy: 0.9784 - val_loss: 0.4600 - val_accuracy: 0.9715\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.0749 - accuracy: 0.9776 - val_loss: 0.4430 - val_accuracy: 0.9715\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.0715 - accuracy: 0.9776 - val_loss: 0.4835 - val_accuracy: 0.9715\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.0715 - accuracy: 0.9784 - val_loss: 0.4425 - val_accuracy: 0.9715\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 0s 328ms/step - loss: 0.0713 - accuracy: 0.9792 - val_loss: 0.4890 - val_accuracy: 0.9715\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.0737 - accuracy: 0.9776 - val_loss: 0.4314 - val_accuracy: 0.9687\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.0741 - accuracy: 0.9768 - val_loss: 0.5141 - val_accuracy: 0.9715\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.0813 - accuracy: 0.9752 - val_loss: 0.3903 - val_accuracy: 0.9687\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 0s 311ms/step - loss: 0.0960 - accuracy: 0.9768 - val_loss: 0.5964 - val_accuracy: 0.9487\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.1165 - accuracy: 0.9760 - val_loss: 0.4754 - val_accuracy: 0.9687\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 0.0963 - accuracy: 0.9679 - val_loss: 0.5761 - val_accuracy: 0.9088\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.0850 - accuracy: 0.9800 - val_loss: 0.4683 - val_accuracy: 0.9687\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.0752 - accuracy: 0.9784 - val_loss: 0.4820 - val_accuracy: 0.9715\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 0s 348ms/step - loss: 0.0691 - accuracy: 0.9784 - val_loss: 0.4753 - val_accuracy: 0.9715\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 0s 306ms/step - loss: 0.0674 - accuracy: 0.9808 - val_loss: 0.4732 - val_accuracy: 0.9715\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.0666 - accuracy: 0.9792 - val_loss: 0.4734 - val_accuracy: 0.9715\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.0660 - accuracy: 0.9816 - val_loss: 0.4743 - val_accuracy: 0.9715\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 0s 298ms/step - loss: 0.0653 - accuracy: 0.9792 - val_loss: 0.4748 - val_accuracy: 0.9715\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.0648 - accuracy: 0.9824 - val_loss: 0.4770 - val_accuracy: 0.9715\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.0643 - accuracy: 0.9808 - val_loss: 0.4776 - val_accuracy: 0.9715\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.0638 - accuracy: 0.9816 - val_loss: 0.4802 - val_accuracy: 0.9715\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.0634 - accuracy: 0.9808 - val_loss: 0.4799 - val_accuracy: 0.9715\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.0631 - accuracy: 0.9808 - val_loss: 0.4839 - val_accuracy: 0.9715\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 0.0632 - accuracy: 0.9808 - val_loss: 0.4787 - val_accuracy: 0.9715\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 0s 315ms/step - loss: 0.0648 - accuracy: 0.9784 - val_loss: 0.4865 - val_accuracy: 0.9715\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.0778 - accuracy: 0.9776 - val_loss: 0.4525 - val_accuracy: 0.9715\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.0694 - accuracy: 0.9800 - val_loss: 0.5346 - val_accuracy: 0.9487\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.0793 - accuracy: 0.9760 - val_loss: 0.4370 - val_accuracy: 0.9687\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.1024 - accuracy: 0.9712 - val_loss: 0.5898 - val_accuracy: 0.9459\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 0s 485ms/step - loss: 0.0922 - accuracy: 0.9752 - val_loss: 0.4695 - val_accuracy: 0.9715\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.5231 - accuracy: 0.9271 - val_loss: 0.4682 - val_accuracy: 0.9715\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 0s 314ms/step - loss: 0.0721 - accuracy: 0.9784 - val_loss: 0.4679 - val_accuracy: 0.9715\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 0.0677 - accuracy: 0.9800 - val_loss: 0.4647 - val_accuracy: 0.9715\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 0s 309ms/step - loss: 0.0665 - accuracy: 0.9808 - val_loss: 0.4622 - val_accuracy: 0.9715\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.0656 - accuracy: 0.9808 - val_loss: 0.4602 - val_accuracy: 0.9715\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.0650 - accuracy: 0.9808 - val_loss: 0.4585 - val_accuracy: 0.9715\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.0644 - accuracy: 0.9808 - val_loss: 0.4570 - val_accuracy: 0.9715\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 0s 333ms/step - loss: 0.0639 - accuracy: 0.9808 - val_loss: 0.4557 - val_accuracy: 0.9687\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 0s 321ms/step - loss: 0.0635 - accuracy: 0.9816 - val_loss: 0.4545 - val_accuracy: 0.9687\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.0631 - accuracy: 0.9816 - val_loss: 0.4534 - val_accuracy: 0.9687\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.0627 - accuracy: 0.9816 - val_loss: 0.4524 - val_accuracy: 0.9687\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 0s 318ms/step - loss: 0.0623 - accuracy: 0.9816 - val_loss: 0.4514 - val_accuracy: 0.9687\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 0s 312ms/step - loss: 0.0619 - accuracy: 0.9824 - val_loss: 0.4505 - val_accuracy: 0.9687\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 0s 300ms/step - loss: 0.0616 - accuracy: 0.9816 - val_loss: 0.4496 - val_accuracy: 0.9687\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.0612 - accuracy: 0.9816 - val_loss: 0.4488 - val_accuracy: 0.9687\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 0s 342ms/step - loss: 0.0609 - accuracy: 0.9808 - val_loss: 0.4480 - val_accuracy: 0.9687\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 0s 322ms/step - loss: 0.0605 - accuracy: 0.9808 - val_loss: 0.4471 - val_accuracy: 0.9687\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 0s 317ms/step - loss: 0.0603 - accuracy: 0.9808 - val_loss: 0.4470 - val_accuracy: 0.9687\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 0s 293ms/step - loss: 0.0611 - accuracy: 0.9824 - val_loss: 0.4450 - val_accuracy: 0.9687\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 0s 301ms/step - loss: 0.0599 - accuracy: 0.9800 - val_loss: 0.4467 - val_accuracy: 0.9687\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.0597 - accuracy: 0.9808 - val_loss: 0.4425 - val_accuracy: 0.9687\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 0s 316ms/step - loss: 0.0605 - accuracy: 0.9792 - val_loss: 0.4541 - val_accuracy: 0.9687\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 0.0600 - accuracy: 0.9800 - val_loss: 0.4414 - val_accuracy: 0.9687\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 0s 303ms/step - loss: 0.0604 - accuracy: 0.9808 - val_loss: 0.4801 - val_accuracy: 0.9715\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 0s 327ms/step - loss: 0.0639 - accuracy: 0.9808 - val_loss: 0.4376 - val_accuracy: 0.9687\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 0s 310ms/step - loss: 0.0691 - accuracy: 0.9776 - val_loss: 0.5133 - val_accuracy: 0.9715\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.0792 - accuracy: 0.9760 - val_loss: 0.4420 - val_accuracy: 0.9687\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 0s 331ms/step - loss: 0.0638 - accuracy: 0.9792 - val_loss: 0.4610 - val_accuracy: 0.9687\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 0s 320ms/step - loss: 0.0594 - accuracy: 0.9808 - val_loss: 0.4496 - val_accuracy: 0.9687\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 0s 307ms/step - loss: 0.0584 - accuracy: 0.9784 - val_loss: 0.4483 - val_accuracy: 0.9687\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 0s 330ms/step - loss: 0.0584 - accuracy: 0.9816 - val_loss: 0.4468 - val_accuracy: 0.9687\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 0s 313ms/step - loss: 0.0583 - accuracy: 0.9800 - val_loss: 0.4466 - val_accuracy: 0.9687\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.0575 - accuracy: 0.9840 - val_loss: 0.4462 - val_accuracy: 0.9687\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 0s 304ms/step - loss: 0.0571 - accuracy: 0.9824 - val_loss: 0.4458 - val_accuracy: 0.9687\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.0569 - accuracy: 0.9832 - val_loss: 0.4455 - val_accuracy: 0.9687\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1OTqvVce7mn"
      },
      "source": [
        "encoder_model = Model(encoder_inputs, encoder_states)\n",
        "\n",
        "decoder_state_input_h = Input(shape=(latent_dim,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim,))\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
        "decoder_states = [state_h, state_c]\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "decoder_model = Model([decoder_inputs] + decoder_states_inputs,[decoder_outputs] + decoder_states)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGphtpyje7mn"
      },
      "source": [
        "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
        "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_CM3B5ce7mo"
      },
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    states_value = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "    # Populate the first character of target sequence with the start character.\n",
        "    target_seq[0, 0, target_token_index['<START>']] = 1.\n",
        "\n",
        "    # Sampling loop for a batch of sequences\n",
        "    # (to simplify, here we assume a batch of size 1).\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "        output_tokens, h, c = decoder_model.predict(\n",
        "            [target_seq] + states_value)\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
        "        decoded_sentence += sampled_char + \" \"\n",
        "\n",
        "        # Exit condition: either hit max length\n",
        "        # or find stop character.\n",
        "        if (sampled_char == '<END>' or\n",
        "           len(decoded_sentence) > max_decoder_seq_length):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
        "        target_seq[0, 0, sampled_token_index] = 1.\n",
        "\n",
        "        # Update states\n",
        "        states_value = [h, c]\n",
        "    return decoded_sentence.strip()"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbgEpczLe7mo",
        "outputId": "0f08824a-4e51-48ec-c048-f379e628f440"
      },
      "source": [
        "for seq_index in range(20):\n",
        "    # Take one sequence (part of the training set)\n",
        "    # for trying out decoding.\n",
        "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
        "    decoded_sentence = decode_sequence(input_seq)\n",
        "    print('-')\n",
        "    print('Input sentence:', input_texts[seq_index])\n",
        "    print('Decoded sentence:', decoded_sentence)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-\n",
            "Input sentence: 창워느로 출짱 가는 나리 언제니 ?\n",
            "Decoded sentence: 내가 이 사달이 날 줄 알았다. <END>\n",
            "-\n",
            "Input sentence: 새해 복 믾이 밪으세요.\n",
            "Decoded sentence: 개방정 좀 그만 떨어라 <END>\n",
            "-\n",
            "Input sentence: 어의가 없네 ?\n",
            "Decoded sentence: 며칠만 기다리면 된대 <END>\n",
            "-\n",
            "Input sentence: 깨방정 좀 그만 떨어라\n",
            "Decoded sentence: 개방정 좀 그만 떨어라 <END>\n",
            "-\n",
            "Input sentence: 왠만하면\n",
            "Decoded sentence: 내로라하는 <END>\n",
            "-\n",
            "Input sentence: 어따 대고 말대꾸야!\n",
            "Decoded sentence: 얻다 대고 말대꾸야! <END>\n",
            "-\n",
            "Input sentence: 오랫만에 만나는구나~\n",
            "Decoded sentence: 나중에 봬요. <END>\n",
            "-\n",
            "Input sentence: 비가 오는 바람에 모임이 파토가 났다.\n",
            "Decoded sentence: 개방정 좀 그만 떨어라 <END>\n",
            "-\n",
            "Input sentence: 나중에 뵈요.\n",
            "Decoded sentence: 나중에 봬요. <END>\n",
            "-\n",
            "Input sentence: 내가 이 사단이 날 줄 알았다.\n",
            "Decoded sentence: 내가 이 사달이 날 줄 알았다. <END>\n",
            "-\n",
            "Input sentence: 어떻해\n",
            "Decoded sentence: 내로라하는 <END>\n",
            "-\n",
            "Input sentence: 넌 내 꺼야!\n",
            "Decoded sentence: 며칠만 기다리면 된대 <END>\n",
            "-\n",
            "Input sentence: 몇일만 기다리면 된대\n",
            "Decoded sentence: 며칠만 기다리면 된대 <END>\n",
            "-\n",
            "Input sentence: 너 그 예기 들었니?\n",
            "Decoded sentence: 내가 이 사달이 날 줄 알았다. <END>\n",
            "-\n",
            "Input sentence: 병이 낳다.\n",
            "Decoded sentence: 담배를 피우다. <END>\n",
            "-\n",
            "Input sentence: 천재라고 불리운다.\n",
            "Decoded sentence: 나중에 봬요. <END>\n",
            "-\n",
            "Input sentence: 담배를 피다.\n",
            "Decoded sentence: 담배를 피우다. <END>\n",
            "-\n",
            "Input sentence: 설겆이\n",
            "Decoded sentence: 내로라하는 <END>\n",
            "-\n",
            "Input sentence: 금새\n",
            "Decoded sentence: 내로라하는 <END>\n",
            "-\n",
            "Input sentence: 회사의 어음결재 관련 서류를 사장이 결제했다.\n",
            "Decoded sentence: 내가 이 사달이 날 줄 알았다. <END>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2anRYi3p2_Ql"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}